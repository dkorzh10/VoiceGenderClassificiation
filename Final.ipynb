{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1pZ7YYCNPUhjlj2aKX24gNlhxZ3ctx6k1\n",
      "To: /workspace/raid/data/dkorzh/TERM 7/TinkSpeech/FinalVersions/cv-corpus-7.0-2021-07-21-ru.tar.gz\n",
      "100%|██████████████████████████████████████| 4.54G/4.54G [02:46<00:00, 27.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "# # dataset downoloading\n",
    "# !gdown --id 1pZ7YYCNPUhjlj2aKX24gNlhxZ3ctx6k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -xvf cv-corpus-7.0-2021-07-21-ru.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final.ipynb  \u001b[0m\u001b[01;34mcv-corpus-7.0-2021-07-21\u001b[0m/  \u001b[01;31mcv-corpus-7.0-2021-07-21-ru.tar.gz\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.12\r\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сделаем скрипт для подготовки датафреймов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile datasets.py\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "#sys.path.append('..')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='DataFramesPreparation')\n",
    "\n",
    "parser.add_argument('--dataset_tsv_path', type=str, help='path to train.tsv and test.tsv', default ='./cv-corpus-7.0-2021-07-21/ru/' )\n",
    "parser.add_argument('--train_name', type=str, help='filename.csv, that will be used for training', default ='train_part.csv' )\n",
    "parser.add_argument('--val_name', type=str, help='filename.csv, that will be used for validation', default ='val_part.csv' )\n",
    "parser.add_argument('--test_name', type=str, help='filename.csv, that will be used for testing', default ='test_dropna.csv' )\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    df_train = pd.read_csv(args.dataset_tsv_path+'train.tsv', sep='\\t')\n",
    "    df_test = pd.read_csv(args.dataset_tsv_path+'test.tsv', sep='\\t')\n",
    "    \n",
    "    df_train = df_train[['client_id','path','gender']]\n",
    "    df_test = df_test[['client_id','path','gender']]\n",
    "    \n",
    "    df_train_female = df_train[df_train.gender=='female'].copy()\n",
    "    df_train_male = df_train[df_train.gender=='male'].sample(3000).copy()\n",
    "    df_train = df_train_female.append(df_train_male)\n",
    "    df_train = df_train.sample(frac=1)\n",
    "    \n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_train['gender'].replace(to_replace=['female', 'male'],value= [0, 1], inplace=True)\n",
    "    \n",
    "    df_test = df_test.dropna()\n",
    "    df_test['gender'].replace(to_replace=['female', 'male'],value= [0, 1], inplace=True)\n",
    "    \n",
    "    df_train.to_csv('train_preproc_balanced.csv')\n",
    "    df_test.to_csv(args.test_name)\n",
    "    \n",
    "    df = pd.read_csv('train_preproc_balanced.csv')\n",
    "    df.dtypes\n",
    "    df = df.sample(5556)\n",
    "    train_df = df[0:4556]\n",
    "    val_df = df[4556:]\n",
    "    \n",
    "    train_df.to_csv(args.train_name)\n",
    "    val_df.to_csv(args.val_name)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python datasets.py --dataset_tsv_path './cv-corpus-7.0-2021-07-21/ru/' --train_name 'train_part.csv' \\\n",
    "--val_name 'val_part.csv' --test_name 'test_dropna.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final.ipynb                         datasets.py      train_preproc_balanced.csv\r\n",
      "\u001b[0m\u001b[01;34mcv-corpus-7.0-2021-07-21\u001b[0m/           test_dropna.csv  val_part.csv\r\n",
      "\u001b[01;31mcv-corpus-7.0-2021-07-21-ru.tar.gz\u001b[0m  train_part.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сделаем файл для подготовки датасета, даталоадера и трейна для лучшей (единственно работающей) модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "#sys.path.append('..')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn.metrics as metrics\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Training')\n",
    "\n",
    "\n",
    "parser.add_argument('--AUDIO_DIR', default=\"./cv-corpus-7.0-2021-07-21/ru/clips/\", type=str,\n",
    "                    help='path to directory with audio clips')\n",
    "\n",
    "parser.add_argument('--ANN_FILE', default='train_preproc_balanced.csv', type=str,\n",
    "                    help='path to directory with audio clips')\n",
    "\n",
    "parser.add_argument('--gpu', default='0', type=str,\n",
    "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "\n",
    "parser.add_argument('--train_name', type=str, help='filename.csv, that will be used for training', default ='train_part.csv' )\n",
    "parser.add_argument('--val_name', type=str, help='filename.csv, that will be used for validation', default ='val_part.csv' )\n",
    "parser.add_argument('--test_name', type=str, help='filename.csv, that will be used for testing', default ='test_dropna.csv' )\n",
    "parser.add_argument('--batch_size', default=16, type=int,help='batchsize (default: 16)')\n",
    "parser.add_argument('--num_workers', default=4, type=int,help='num dataloader workers (0 or greater)')\n",
    "\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.001, type=float,\n",
    "                    help='initial learning rate', dest='lr')\n",
    "\n",
    "parser.add_argument('--momentum', default=0.9, type=float,\n",
    "                    help='sgd momentum',)\n",
    "\n",
    "parser.add_argument('--epochs', default=3, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--best_model_name', type=str, help='filename (without .pt),to save best model on validation', default ='MelsSpecsResnetBest')\n",
    "args = parser.parse_args()\n",
    "\n",
    "#default values, will declared explicitly\n",
    "AUDIO_DIR = args.AUDIO_DIR\n",
    "ANN_FILE = args.ANN_FILE\n",
    "\n",
    "if args.gpu:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print(device) \n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=3, scheduler= None,\n",
    "                savename='best_model', is_inception=False):\n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                \n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            if phase=='train':\n",
    "                scheduler.step()\n",
    "                \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), savename+'.pt')\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, val_acc_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomAudioDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_dir = AUDIO_DIR, annotation_file = ANN_FILE, resample_freq =16000,\n",
    "                 transform=None, target_transform=None, n_mels = 64):\n",
    "        df = pd.read_csv(annotation_file)\n",
    "        self.audio_labels = df[['path', 'gender']].copy() #pd.read_csv(annotation_file, names=['paths', 'gender'])\n",
    "        #print(self.audio_labels)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.resample = resample_freq\n",
    "        self.n_mels = n_mels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        label = self.audio_labels.iloc[idx, 1]\n",
    "        #print(label)\n",
    "        #label = int(label)\n",
    "        label = torch.tensor(label)\n",
    "        \n",
    "        audio_path = os.path.join(self.audio_dir, self.audio_labels.iloc[idx, 0])\n",
    "        waveform , sr = torchaudio.load(audio_path)\n",
    "\n",
    "        \n",
    "        if self.resample > 0:\n",
    "            resample_transform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.resample)\n",
    "            waveform = resample_transform(waveform)\n",
    "    \n",
    "        melspectrogram_transform = torchaudio.transforms.MelSpectrogram(sample_rate=self.resample, n_mels=self.n_mels)\n",
    "        melspectrogram = melspectrogram_transform(waveform)\n",
    "        melspectogram_db = torchaudio.transforms.AmplitudeToDB()(melspectrogram)\n",
    "        #print(melspectogram_db.shape)\n",
    "        #Make sure all spectrograms are the same size\n",
    "        fixed_length = 3 * (self.resample//100) #//200\n",
    "        if melspectogram_db.shape[2] < fixed_length:\n",
    "            melspectogram_db = torch.nn.functional.pad(\n",
    "              melspectogram_db, (0, fixed_length - melspectogram_db.shape[2]))\n",
    "        else:\n",
    "            melspectogram_db = melspectogram_db[:, :, :fixed_length]\n",
    "\n",
    "        return melspectogram_db, label #soundData, self.resample, melspectogram_db, self.labels[index]\n",
    "        \n",
    "#         return specgram, label\n",
    "\n",
    "def test(model, criterion):\n",
    "    test_dataset = CustomAudioDataset(annotation_file = args.test_name)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "    \n",
    "    model = model.eval()\n",
    "    phase = 'test'\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    y = []\n",
    "    y_pred_0 = []\n",
    "    y_pred_1 = []\n",
    "    for inputs, labels in tqdm(test_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        y.append(list(labels.view(-1).detach().cpu()))\n",
    "\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            m = nn.Softmax(dim=1)\n",
    "            q = m(outputs)\n",
    "\n",
    "            y_pred_0.append(list(q[:,0].detach().cpu()))\n",
    "            y_pred_1.append(list(q[:,1].detach().cpu()))\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(test_dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(test_dataloader.dataset)\n",
    "\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "    \n",
    "    def flatten(t):\n",
    "        return [float(item) for sublist in t for item in sublist]\n",
    "\n",
    "    y = flatten(y)\n",
    "    y_pred_1 = flatten(y_pred_1)\n",
    "    \n",
    "    print(\"ROC-AUC score on test dataset:\", round(roc_auc_score(y, y_pred_1), 4))\n",
    "    \n",
    "    preds = y_pred_1\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.savefig('roc_auc.pdf')\n",
    "    plt.show()\n",
    "\n",
    "def train():\n",
    "    \n",
    "    \n",
    "    train_dataset = CustomAudioDataset(annotation_file = args.train_name)\n",
    "    val_dataset = CustomAudioDataset(annotation_file = args.val_name)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "    \n",
    "    \n",
    "    \n",
    "    classes =2 \n",
    "\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.conv1=nn.Conv2d(1, model.conv1.out_channels, \n",
    "                          kernel_size=model.conv1.kernel_size[0], \n",
    "                          stride=model.conv1.stride[0], \n",
    "                          padding=model.conv1.padding[0])\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, classes)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = args.lr, momentum = args.momentum)\n",
    "    #don't need scheduler, one can add arguments to parser\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 8, gamma = 0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    dataloaders = {'train': train_dataloader, 'val': val_dataloader}\n",
    "    \n",
    "    model, val_acc_history = train_model(model, dataloaders, criterion, optimizer, num_epochs=args.epochs,scheduler= scheduler, \n",
    "            savename=args.best_model_name, is_inception=False)\n",
    "    \n",
    "\n",
    "    test(model,criterion)\n",
    "    \n",
    "    \n",
    "if __name__== \"__main__\":\n",
    "    train()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 0/2\n",
      "----------\n",
      "100%|█████████████████████████████████████████| 285/285 [00:14<00:00, 19.24it/s]\n",
      "train Loss: 0.1361 Acc: 0.9497\n",
      "100%|███████████████████████████████████████████| 63/63 [00:03<00:00, 17.60it/s]\n",
      "val Loss: 0.0602 Acc: 0.9850\n",
      "\n",
      "Epoch 1/2\n",
      "----------\n",
      "100%|█████████████████████████████████████████| 285/285 [00:14<00:00, 19.16it/s]\n",
      "train Loss: 0.0385 Acc: 0.9875\n",
      "100%|███████████████████████████████████████████| 63/63 [00:03<00:00, 17.88it/s]\n",
      "val Loss: 0.0190 Acc: 0.9940\n",
      "\n",
      "Epoch 2/2\n",
      "----------\n",
      "100%|█████████████████████████████████████████| 285/285 [00:14<00:00, 19.16it/s]\n",
      "train Loss: 0.0174 Acc: 0.9954\n",
      "100%|███████████████████████████████████████████| 63/63 [00:03<00:00, 17.25it/s]\n",
      "val Loss: 0.0143 Acc: 0.9960\n",
      "\n",
      "Training complete in 0m 56s\n",
      "Best val Acc: 0.996000\n",
      "100%|█████████████████████████████████████████| 241/241 [00:13<00:00, 17.81it/s]\n",
      "test Loss: 0.2278 Acc: 0.9393\n",
      "ROC-AUC score on test dataset: 0.9681\n",
      "Figure(640x480)\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final.ipynb                         roc_auc.pdf\r\n",
      "MelsSpecsResnetBest.pt              test_dropna.csv\r\n",
      "\u001b[0m\u001b[01;34mcv-corpus-7.0-2021-07-21\u001b[0m/           train.py\r\n",
      "\u001b[01;31mcv-corpus-7.0-2021-07-21-ru.tar.gz\u001b[0m  train_part.csv\r\n",
      "datasets.py                         train_preproc_balanced.csv\r\n",
      "requirements.txt                    val_part.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5b86163420297e1308e6ff0026894af5  MelsSpecsResnetBest.pt\r\n"
     ]
    }
   ],
   "source": [
    "!md5sum MelsSpecsResnetBest.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* https://medium.com/analytics-vidhya/how-to-classify-sounds-using-pytorch-27c9f2d4d714\n",
    "* https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "* https://towardsdatascience.com/audio-classification-with-pytorchs-ecosystem-tools-5de2b66e640c\n",
    "* https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-a-list-of-lists\n",
    "* https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n",
    "* https://stackoverflow.com/questions/53633177/how-to-read-a-mp3-audio-file-into-a-numpy-array-save-a-numpy-array-to-mp3\n",
    "* https://towardsdatascience.com/audio-classification-with-pytorchs-ecosystem-tools-5de2b66e640c\n",
    "* https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
